# -*- coding: utf-8 -*-
"""Copy of Knightec_CNN_LeNet_v2x.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mdoOIYUVtD23R1ImogoHJ_Mkcxpz9KI9

Using Enric's data with CNN LeNet model
- sampling rate is 5kHz
- 


- will use 12kHz files since the normal data is in 12kHz
- images are 16*16 since 12kHz have less samples
- loading from CWRU_16.pickle
- normalizing data between [0,1] instead of [0,255]
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard


import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid
from scipy import signal
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions
from tensorflow.keras.applications.xception import Xception
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2

import os
import datetime

from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.utils import class_weight

import copy

import neptune.new as neptune
from neptune.new.integrations.tensorflow_keras import NeptuneCallback
#import neptune.new.integrations.sklearn as npt_utils

#import scikitplot as skplt
#from scikitplot.metrics import plot_confusion_matrix, plot_roc


## GLOBAL SEED ##   
#np.random.seed(1)                                                
#tf.random.set_seed(1)
sklearn_seed = 42
print(tf.version.VERSION)

M = 64
sample_rate = 5000

# work directory of all data
work_dir = os.getcwd()
rdir = os.path.join(os.path.expanduser(work_dir), 'Datasets')

# load all data previously pickled (preprocess.py)
filename = 'knightec_100_5000_{}.pickle'.format(M)
with open(os.path.join(rdir, filename), 'rb') as f:
  data = pickle.load(f)

thesis_dir = os.getcwd()
current_dir = "5_Knightec_CNN_LeNet"
file_name = "Knightec_CNN_LeNet_1"
rdir = os.path.join(os.path.expanduser(thesis_dir), current_dir)

"""Split training set into train / validation.
NOTE! Will skip this step as it is automatically done when training the model.
"""

# Split training data into training and validatiom
print(data.X_v2x_train.shape)
data.X_v2x_train, data.X_v2x_val, data.y_train, data.y_val = train_test_split(data.X_v2x_train, data.y_train, 
                                                                      test_size=0.2, random_state=sklearn_seed, 
                                                                      shuffle=False, stratify=None)
print(data.X_v2x_train.shape)
print(data.X_v2x_val.shape)

slice_size = M*M
T = 1.0 / sample_rate
N = (2 - 0) * sample_rate
time = np.linspace(0, 2, N)

# Use DataFrame for better manipulation
#data_columns = ["0.007B", "0.007IR", "0.007OR12", "0.007OR3", "0.007OR6", "0.014B", "0.014IR", "0.014OR6", "0.021B", "0.021IR", "0.021OR12", "0.021OR3", "0.021OR6", "Normal"]
df = pd.DataFrame(data.X_v2x_val[0])
df.index.name = "Cycles"
# print(df)

def to_image(df, M):
    """
    single image conversion from dataframe[0:M*M] to M*M ndarray
    :param df:
    :param M:
    :return:
    """
    P = df.values.reshape(M, M)
    #P = np.round((P-np.min(df.values)/(np.max(df.values)-np.min(df.values)))*255)
    P = (P-np.min(df.values))/(np.max(df.values)-np.min(df.values))
    return P

# Convert dataframe element to image for testing purposes
data_to_convert = df[0:(M*M)]
data_image = to_image(data_to_convert, M)
# plt.imshow(data_image, cmap='gray')
plt.imshow(data_image)
plt.show()

# data_image

def to_images(data):
    """
    Converts time-series data into normalized images
    :param data:
    :return:
    """
    min_val = np.amin(data, axis=1).reshape(-1, 1)
    max_val = np.amax(data, axis=1).reshape(-1, 1)
    #P = np.around(((data-min_val) / (max_val-min_val)) * 255)
    P = data-min_val / (max_val-min_val)
    return P

def show_images(images, labels):
    """
    Creates a collage all class images
    :param images: all images data. ndarray of 64x64 elements
    :param labels: labels of images. 0-13
    :return: none
    """

    image_index, unique_img = [], []
    for i in range(0, len(data.labels)):
        image_index.append(labels.index(i))
        unique_img.append(images[labels.index(i)].squeeze(axis=2))

    fig = plt.figure(figsize=(10, 10))
    grid = ImageGrid(fig, 111,  # similar to subplot(111)
                     nrows_ncols=(int(len(data.labels)/2), 3),  # creates 3x2 grid of axes
                     axes_pad=0.1,  # pad between axes in inch.
                     )
    for i, (ax, im) in enumerate(zip(grid, unique_img)):
        # Iterating over the grid returns the Axes.
        ax.imshow(im)
        ax.set_title(data.labels[i])

    plt.show()

# convert all time-series slices to images
train_X = np.array([im.reshape(M, M) for im in to_images(data.X_v2x_train)])
test_X = np.array([im.reshape(M, M) for im in to_images(data.X_v2x_test)])
val_X = np.array([im.reshape(M, M) for im in to_images(data.X_v2x_val)])

# add channel for grayscale
train_X = train_X[..., np.newaxis]
test_X = test_X[..., np.newaxis]
val_X = val_X[..., np.newaxis]

#display some of the images
show_images(train_X, data.y_train)

train_X[0].shape

# this function can be replaced with Keras builtin to_categorical() function.

def OneHot(y):
    """
        input:  y (n) = labels
        output: onehot (Kxn) = one-hot representation of labels
    """
    onehot = np.zeros((len(data.labels), len(y)))
    for idx, val in enumerate(y):
        onehot[val][idx] = 1
    # print("oneshot {}".format(onehot))
    return onehot

# convert labels to one-hot encoding
train_y = OneHot(data.y_train).swapaxes(1, 0)
test_y = OneHot(data.y_test).swapaxes(1, 0)
val_y = OneHot(data.y_val).swapaxes(1, 0)

# https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras
class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(np.argmax(train_y, axis=1)),
                                                 np.argmax(train_y, axis=1))
class_weight_dict = dict(enumerate(class_weights))

"""# Lenet-based model based on Paper 2"""

FD1 = 2560
FD2 = 768

def lenet_64():
    """
    model based on LeNet-5
    activation function is tanh
    :return:
    """
    model = models.Sequential(name='lenet')
    model.add(layers.Conv2D(filters=32, kernel_size=5, input_shape=(M, M, 1), activation="tanh", padding="same", name="L1"))
    model.add(layers.MaxPool2D(pool_size=2, name="L2"))
    model.add(layers.Conv2D(filters=64, kernel_size=3, activation="tanh", padding="same", name="L3"))
    model.add(layers.MaxPool2D(pool_size=2, name="L4"))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation="tanh", padding="same", name="L5"))
    model.add(layers.MaxPool2D(pool_size=2, name="L6"))
    model.add(layers.Conv2D(filters=256, kernel_size=3, activation="tanh", padding="same", name="L7"))
    model.add(layers.MaxPool2D(pool_size=2, name="L8"))
    model.add(layers.Flatten())
    model.add(layers.Dense(2560, activation='tanh'))
    #model.add(layers.Dropout(0.2))
    model.add(layers.Dense(768, activation='tanh'))
    #model.add(layers.Dropout(0.2))
    model.add(layers.Dense(len(data.labels), activation='softmax'))
    #print(model.summary())

    return model

def lenet_32():
    """
    model based on LeNet-5
    activation function is tanh
    :return:
    """
    model = models.Sequential(name='lenet')
    model.add(layers.Conv2D(filters=64, kernel_size=3, input_shape=(M, M, 1), activation="tanh", padding="same", name="L3"))
    model.add(layers.MaxPool2D(pool_size=2, name="L4"))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation="tanh", padding="same", name="L5"))
    model.add(layers.MaxPool2D(pool_size=2, name="L6"))
    model.add(layers.Conv2D(filters=256, kernel_size=3, activation="tanh", padding="same", name="L7"))
    model.add(layers.MaxPool2D(pool_size=2, name="L8"))
    model.add(layers.Flatten())
    model.add(layers.Dense(2560, activation='tanh'))
    #model.add(layers.Dropout(0.2))
    model.add(layers.Dense(768, activation='tanh'))
    #model.add(layers.Dropout(0.2))
    model.add(layers.Dense(len(data.labels), activation='softmax'))
    #print(model.summary())
    return model

def lenet_16():
    """
    model based on LeNet-5
    activation function is tanh
    :return:
    """
    model = models.Sequential(name='lenet')
    model.add(layers.Conv2D(filters=128, kernel_size=3, input_shape=(M, M, 1), activation="tanh", padding="same", name="L5"))
    model.add(layers.MaxPool2D(pool_size=2, name="L6"))
    model.add(layers.Conv2D(filters=256, kernel_size=3, activation="tanh", padding="same", name="L7"))
    model.add(layers.MaxPool2D(pool_size=2, name="L8"))
    model.add(layers.Flatten())
    model.add(layers.Dense(2560, activation='tanh'))
    #model.add(layers.Dropout(0.2))
    model.add(layers.Dense(768, activation='tanh'))
    #model.add(layers.Dropout(0.2))
    model.add(layers.Dense(len(data.labels), activation='softmax'))
    #print(model.summary())
    return model

if M==64:
  lenet = lenet_64()
elif M==32:
  lenet = lenet_32()
else:
  lenet = lenet_16()

"""# Transfer Learning using Keras Applications

## VGG16
"""

base_VGG16 = VGG16(include_top=False, weights="imagenet", 
                   input_tensor=layers.Input(shape=(M, M, 3)),
                   classes=len(data.labels))

# add a global spatial average pooling layer
x = base_VGG16.output
x = layers.GlobalAveragePooling2D()(x)
# add a fully-connected layer
x = layers.Dense(1024, activation='relu')(x)
# and a logistic layer -- let's say we have 7 classes
predictions = layers.Dense(len(data.labels), activation='tanh')(x) 
VGG16 = models.Model(inputs=base_VGG16.input, outputs=predictions, name='vgg16')
#VGG16.summary()

"""# MobileNet2"""

base_MobileNetV2 = MobileNetV2(include_top=False, 
                         input_tensor=layers.Input(shape=(M,M,3)), 
                         classes=len(data.labels))
# add a global spatial average pooling layer
x = base_MobileNetV2.output
x = layers.GlobalAveragePooling2D()(x)
# add a fully-connected layer
x = layers.Dense(1024, activation='relu')(x)
# and a logistic layer -- let's say we have 7 classes
predictions = layers.Dense(len(data.labels), activation='tanh')(x) 
MobileNetV2 = models.Model(inputs=base_MobileNetV2.input, outputs=predictions, 
                           name='MobileNetV2')
#MobileNetV2.summary()

"""# XCeption"""

base_Xception = Xception(include_top=False, 
                         input_tensor=layers.Input(shape=(M,M,3)), 
                         classes=len(data.labels))
# add a global spatial average pooling layer
x = base_Xception.output
x = layers.GlobalAveragePooling2D()(x)
# add a fully-connected layer
x = layers.Dense(1024, activation='relu')(x)
# and a logistic layer -- let's say we have 7 classes
predictions = layers.Dense(len(data.labels), activation='tanh')(x) 
Xception = models.Model(inputs=base_Xception.input, outputs=predictions, 
                        name='Xception')
#Xception.summary()

"""# Callbacks"""

def create_callbacks(exp, neptune_run):
  # Callbacks, early stopping and best model restoration
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy',
                                                    verbose=1,
                                                    patience=30,
                                                    mode='max',
                                                    restore_best_weights=True)
  # Neptune
  neptune_cbk = NeptuneCallback(run=neptune_run, base_namespace='metrics')

  # Tensorboard directory and callback function
  log_dir = "logs/fit_{}/".format(exp) + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

  return [early_stopping, neptune_cbk, tb_callback]

def plot_loss(model, history):
  # Plot results
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label = 'val_loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  # plt.ylim([0, 0.1])
  plt.legend(loc='lower right')
  plt.title("{}".format(model.name))
  plt.show()

def plot_accuracy(model, history):
  # Plot results
  plt.plot(history.history['accuracy'], label='accuracy')
  plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  # plt.ylim([0, 0.1])
  plt.legend(loc='lower right')
  plt.title("{}".format(model.name))
  plt.show()

def get_predictions(model, data, X, y):
  # Predict validation data
  y_pred = np.argmax(model.predict(X), axis=1)
  #print("Validation set prediction:")
  conf_matrix = confusion_matrix(y.argmax(axis=1), y_pred, labels=[i for i, j in enumerate(data.labels)])
  print(conf_matrix)
  class_report = classification_report(y.argmax(axis=1), y_pred, digits=3, output_dict=True)
  class_report2 = classification_report(y.argmax(axis=1), y_pred, digits=3)
  print(class_report2)
  return y_pred, conf_matrix, class_report

def get_conf_matrix(model, data, conf_matrix):
  df_cm = pd.DataFrame(conf_matrix, index=range(len(data.labels)),
                     columns=range(len(data.labels)))
  df_cm.index.name = 'Actual'
  df_cm.columns.name = 'Predicted'
  plt.figure(figsize = (10,7))
  heatmap = sns.heatmap(df_cm, annot=True, fmt='d', 
              xticklabels=data.labels, 
              yticklabels=data.labels)
  fig = heatmap.get_figure()
  plt.title("{}".format(model.name))
  plt.show()
  return fig

def log_to_neptune(model, data, run, conf, report, tag):
  run['confusion_matrix_{}'.format(tag)] = neptune.types.File.as_image(get_conf_matrix(model, data, conf))
  run['class_report_{}'.format(tag)] = report

def run_model(model, data, train_X, train_y, val_X, val_y, test_X, test_y, PARAMS):
  print("Running {} with hyperparameters {}".format(model.name, PARAMS))
  
  # Neptune
  run = neptune.init(project='diegotheairwolf/knightec', 
                   api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjNzMzOGQ2Yi03YzdjLTRkMWUtYTY1Yi1mNjlhNGY4MjllYzAifQ==',
                   name=model.name, tags=model.name)
  run["hyper-parameters"] = PARAMS
  
  if model.name!='lenet':
    # add 3 grayscale channels
    # https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images/51996037
    X_train = np.repeat(train_X, 3, axis=3)
    X_test = np.repeat(test_X, 3, axis=3)
    X_val = np.repeat(val_X, 3, axis=3)
  else:
    X_train = train_X
    X_test = test_X
    X_val = val_X
  print(X_train.shape)

  model.compile(optimizer=PARAMS["optimizer"],
              loss=PARAMS["loss"],
              metrics=metrics)
  callbacks = create_callbacks(model.name, run)
  
  history = model.fit(X_train, train_y, batch_size=PARAMS["batch_size"],
                      epochs=PARAMS["epochs"],
                      callbacks=callbacks, 
                      steps_per_epoch = int(len(train_y) / PARAMS["batch_size"]), 
                      validation_data=(X_val, val_y),
                      class_weight=class_weight_dict)
  # view/log plots and predictions
  plot_loss(model, history)
  plot_accuracy(model, history)
  val_y_pred, conf_val, report_val = get_predictions(model, data, X_val, val_y)
  test_y_pred, conf_test, report_test= get_predictions(model, data, X_test, test_y)

  log_to_neptune(model, data, run, conf_val, report_val, "validation")
  log_to_neptune(model, data, run, conf_test, report_test, "test")

  # The evaluate() method - gets the loss statistics
  #model.evaluate(test_X, test_y) 
  

# This may generate warnings related to saving the state of the optimizer.
# These warnings (and similar warnings throughout this notebook)
# are in place to discourage outdated usage, and can be ignored.

"""# Hyperparameters"""

# Set hyperparameters and optimizer
batch_size = [1, 2, 4, 6, 16, 32, 64]
learning_rate = [1e-2, 1e-3, 1e-4, 2e-4]
epochs = [10, 100, 1000, 2000]
loss = tf.keras.losses.CategoricalCrossentropy()
metrics = ['accuracy']

"""# Run the Models"""

# for idx, model in enumerate(models):
#     optimizer = keras.optimizers.Adam(learning_rate)
#     PARAMS = {"batch_size": batch_size,
#               "learning_rate": learning_rate,
#               "epochs": epochs,
#               "optimizer": optimizer,
#               "loss": loss,
#               "metrics": metrics}
# 
#     run_model(model, data, train_X, train_y, val_X, val_y, test_X, test_y, PARAMS)

#models = [Xception, lenet, VGG16, MobileNetV2]

models = [lenet, VGG16, MobileNetV2]


for b_size in batch_size:
  for l_r in learning_rate:
    for e in epochs:
      for idx, model in enumerate(models):
        optimizer = keras.optimizers.Adam(l_r)
        PARAMS = {"batch_size":b_size,
                "learning_rate":l_r,
                "epochs":e,
                "optimizer":optimizer,
                "loss":loss,
                "metrics":metrics}
        run_model(model, data, train_X, train_y, val_X, val_y, test_X, test_y, PARAMS)

#%tensorboard --logdir logs/fit os.path(logdir)

# # Save the entire model to a HDF5 file.
# # The '.h5' extension indicates that the model should be saved to HDF5.
# # current date and time
# now = datetime.now()
# timestamp = datetime.timestamp(now)
# model_name = timestamp
# model.save('{}.h5'.format(timestamp))